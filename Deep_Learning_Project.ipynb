{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# URL to the data\n",
        "url = 'https://raw.githubusercontent.com/NiceVincent/NLP_group5/master/data/large/Gift_Cards.json'"
      ],
      "metadata": {
        "id": "2XL0BnvwZxri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "from scipy.spatial import distance\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xml.etree import ElementTree\n",
        "import warnings\n",
        "import shutil\n",
        "import random"
      ],
      "metadata": {
        "id": "k21Wi-L5Uive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)"
      ],
      "metadata": {
        "id": "HIyJ4CZJU7VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_directory = '../input/face-mask-detection/annotations'\n",
        "images_directory = '../input/face-mask-detection/images'"
      ],
      "metadata": {
        "id": "SjjDAhHfU7_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary to store annotation information\n",
        "information = {'xmin': [], 'ymin': [], 'xmax': [], 'ymax': [], 'label': [], 'file': [], 'width': [], 'height': []}"
      ],
      "metadata": {
        "id": "n-vq0hiXU_in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing XML annotations\n",
        "for annotation in glob.glob(annotations_directory + '/*.xml'):\n",
        "    tree = ElementTree.parse(annotation)\n",
        "    for element in tree.iter():\n",
        "        if 'size' in element.tag:\n",
        "            for attribute in list(element):\n",
        "                if 'width' in attribute.tag:\n",
        "                    width = int(round(float(attribute.text)))\n",
        "                if 'height' in attribute.tag:\n",
        "                    height = int(round(float(attribute.text)))\n",
        "\n",
        "        if 'object' in element.tag:\n",
        "            for attribute in list(element):\n",
        "                if 'name' in attribute.tag:\n",
        "                    name = attribute.text\n",
        "                    information['label'] += [name]\n",
        "                    information['width'] += [width]\n",
        "                    information['height'] += [height]\n",
        "                    information['file'] += [annotation.split('/')[-1][0:-4]]\n",
        "\n",
        "                if 'bndbox' in attribute.tag:\n",
        "                    for dimension in list(attribute):\n",
        "                        if 'xmin' in dimension.tag:\n",
        "                            xmin = int(round(float(dimension.text)))\n",
        "                            information['xmin'] += [xmin]\n",
        "                        if 'ymin' in dimension.tag:\n",
        "                            ymin = int(round(float(dimension.text)))\n",
        "                            information['ymin'] += [ymin]\n",
        "                        if 'xmax' in dimension.tag:\n",
        "                            xmax = int(round(float(dimension.text)))\n",
        "                            information['xmax'] += [xmax]\n",
        "                        if 'ymax' in dimension.tag:\n",
        "                            ymax = int(round(float(dimension.text)))\n",
        "                            information['ymax'] += [ymax]"
      ],
      "metadata": {
        "id": "WtwfNA_AVC-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrame from annotation information\n",
        "annotations_info_df = pd.DataFrame(information)\n",
        "\n",
        "# Adding Annotation and Image File Names\n",
        "annotations_info_df['annotation_file'] = annotations_info_df['file'] + '.xml'\n",
        "annotations_info_df['image_file'] = annotations_info_df['file'] + '.png'\n",
        "\n",
        "# Create the 'cropped_image_file' column\n",
        "annotations_info_df['cropped_image_file'] = annotations_info_df['file']\n",
        "\n",
        "# Correcting label grammatical issue\n",
        "annotations_info_df.loc[annotations_info_df['label'] == 'mask_weared_incorrect', 'label'] = 'mask_incorrectly_worn'"
      ],
      "metadata": {
        "id": "btD4ZKp5VE7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to render annotated images\n",
        "def render_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    img = image_path.split('/')[-1]\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    bound_box = []\n",
        "\n",
        "    for i in annotations_info_df[annotations_info_df['image_file'] == img].index:\n",
        "        (xmin, ymin, xmax, ymax) = (annotations_info_df.loc[i].xmin, annotations_info_df.loc[i].ymin,\n",
        "                                     annotations_info_df.loc[i].xmax, annotations_info_df.loc[i].ymax)\n",
        "        bound_box.append((xmin, ymin, xmax, ymax))\n",
        "\n",
        "        if annotations_info_df.loc[i].label == 'with_mask':\n",
        "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 200, 0), 2)\n",
        "            cv2.putText(image, org=(xmin - 8, ymin - 8), text=\"Mask\",\n",
        "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0, 200, 0))\n",
        "        elif annotations_info_df.loc[i].label == 'mask_incorrectly_worn':\n",
        "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 255, 0), 2)\n",
        "            cv2.putText(image, org=(xmin - 8, ymin - 3), text='Incorrect',\n",
        "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(255, 255, 0))\n",
        "        else:\n",
        "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (200, 0, 0), 2)\n",
        "            cv2.putText(image, org=(xmin - 8, ymin - 3), text='No mask',\n",
        "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(200, 0, 0))\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "    return bound_box, image"
      ],
      "metadata": {
        "id": "JHn5aZ5qVvFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing images with two annotations\n",
        "images_with_2 = []\n",
        "for _ in annotations_info_df['image_file'].value_counts().index:\n",
        "    if annotations_info_df[annotations_info_df['image_file'] == _].shape[0] == 2:\n",
        "        images_with_2.append(_)"
      ],
      "metadata": {
        "id": "z7_KmA0AVwmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying images with two annotations\n",
        "for i in images_with_2[:3]:\n",
        "    render_image(os.path.join(images_directory, i))"
      ],
      "metadata": {
        "id": "uhh-DDs9VxnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting bounding boxes and rendering cropped images\n",
        "bound_box, image = render_image(os.path.join(images_directory, i))\n",
        "for i in bound_box:\n",
        "    cropped = image[i[1]:i[3], i[0]:i[2]]\n",
        "    plt.imshow(cropped)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "g7n2x78bVzU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating directory for cropped images\n",
        "directory = 'cropped_images'\n",
        "parent_directory = '/kaggle/working'\n",
        "path = os.path.join(parent_directory, directory)\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(parent_directory)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "os.mkdir(path)"
      ],
      "metadata": {
        "id": "7-oxNuimV01Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cropping and saving images\n",
        "print(\"(0) files are generated.\", end=\"\\r\")\n",
        "for i in range(len(annotations_info_df)):\n",
        "    # Get The File Path and Read The Image\n",
        "    image_filepath = '../input/face-mask-detection/images/' + annotations_info_df['image_file'].iloc[i]\n",
        "    image = cv2.imread(image_filepath)\n",
        "\n",
        "    # Set The Cropped Image File Name\n",
        "    annotations_info_df['cropped_image_file'].iloc[i] = annotations_info_df['cropped_image_file'].iloc[i] + '-' + str(i) + '.png'\n",
        "    cropped_image_filename = annotations_info_df['cropped_image_file'].iloc[i]\n",
        "\n",
        "    # Get The xmin, ymin, xmax, ymax Value (Bounding Box) to Crop Image\n",
        "    xmin = annotations_info_df['xmin'].iloc[i]\n",
        "    ymin = annotations_info_df['ymin'].iloc[i]\n",
        "    xmax = annotations_info_df['xmax'].iloc[i]\n",
        "    ymax = annotations_info_df['ymax'].iloc[i]\n",
        "\n",
        "    # Crop The Image Based on The Values Above\n",
        "    cropped_image = image[ymin:ymax, xmin:xmax]\n",
        "\n",
        "    cropped_image_directory = os.path.join('./cropped_images', cropped_image_filename)\n",
        "    cv2.imwrite(cropped_image_directory, cropped_image)\n",
        "    print(\"{} of {} files are generated.\".format(i + 1, len(annotations_info_df)), end=\"\\r\")"
      ],
      "metadata": {
        "id": "UMBm_gOKV2bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into train and test sets\n",
        "classes = annotations_info_df['label'].unique()\n",
        "labels = annotations_info_df['label']\n",
        "annotations_info_df.drop(['label'], axis=1, inplace=True)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(annotations_info_df, labels, test_size=0.25, stratify=labels,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "nbjEi9RjV302"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying train and test set shapes\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "8fxjbHIyV5E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying train set label counts\n",
        "Y_train.value_counts()"
      ],
      "metadata": {
        "id": "Z995jQGVV6yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect image dimensions\n",
        "image_width = []\n",
        "image_height = []\n",
        "for i in range(len(X_train)):\n",
        "    cropped_image_path = './cropped_images/' + X_train['cropped_image_file'].iloc[i]\n",
        "    cropped_image = cv2.imread(cropped_image_path)\n",
        "    image_width.append(cropped_image.shape[0])\n",
        "    image_height.append(cropped_image.shape[1])"
      ],
      "metadata": {
        "id": "0Q03tpyuV8A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print statistics about image dimensions\n",
        "print('IMAGE WIDTH')\n",
        "print(f'Min: {min(image_width)}')\n",
        "print(f'Max: {max(image_width)}')\n",
        "print(f'Mean: {np.mean(image_width)}')\n",
        "print(f'Median: {np.median(image_width)}')\n",
        "print('IMAGE HEIGHT')\n",
        "print(f'Min: {min(image_height)}')\n",
        "print(f'Max: {max(image_height)}')\n",
        "print(f'Mean: {np.mean(image_height)}')\n",
        "print(f'Median: {np.median(image_height)}')"
      ],
      "metadata": {
        "id": "voXrP4StV9GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set target image size\n",
        "image_target_size = (60, 60)"
      ],
      "metadata": {
        "id": "T7G4xLdGV-RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign labels to training and testing sets\n",
        "X_train['label'] = Y_train\n",
        "X_test['label'] = Y_test"
      ],
      "metadata": {
        "id": "LStjxAi0V_Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image data generator for training\n",
        "train_image_generator = ImageDataGenerator(rescale=1. / 255.)\n",
        "train_generator = train_image_generator.flow_from_dataframe(\n",
        "    dataframe=X_train,\n",
        "    directory='./cropped_images',\n",
        "    x_col='cropped_image_file',\n",
        "    y_col='label',\n",
        "    subset='training',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode='categorical',\n",
        "    target_size=image_target_size\n",
        ")"
      ],
      "metadata": {
        "id": "B8dmYIlQWAKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image data generator for testing\n",
        "test_image_generator = ImageDataGenerator(rescale=1. / 255.)\n",
        "test_generator = train_image_generator.flow_from_dataframe(\n",
        "    dataframe=X_test,\n",
        "    directory='./cropped_images',\n",
        "    x_col='cropped_image_file',\n",
        "    y_col='label',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    class_mode='categorical',\n",
        "    target_size=image_target_size\n",
        ")"
      ],
      "metadata": {
        "id": "6dyiJKyYWBqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model architecture\n",
        "def my_model():\n",
        "    inputs = keras.Input(shape=(60, 60, 3))\n",
        "    x = layers.Conv2D(32, 3)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Conv2D(64, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Conv2D(128, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(3, activation='softmax')(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "rNMrPKxBWC8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = my_model()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "XcXX7GMpWEDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "iKEC0LOMWFlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history_1 = model.fit(train_generator, epochs=10, steps_per_epoch=len(train_generator),\n",
        "                      validation_data=test_generator, validation_steps=len(test_generator))"
      ],
      "metadata": {
        "id": "ZY2IxFGuWGuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an example image and preprocess it for prediction\n",
        "cropped_image_directory = \"/kaggle/input/face-mask-detection/images\"\n",
        "files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "cropped_image_file = os.path.join(directory, random.choice(files))\n",
        "\n",
        "image = cv2.imread(cropped_image_file)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(image)\n",
        "bigger = cv2.resize(image, (60, 60))\n",
        "bigger = bigger / 255\n",
        "bigger = bigger.reshape(1, 60, 60, 3)\n",
        "\n",
        "# Make predictions\n",
        "pred_val = np.argmax(model.predict(bigger))\n",
        "class_ind=train_generator.class_indices\n",
        "\n",
        "# Map prediction index to label\n",
        "y_pred = []\n",
        "for i, j in class_ind.items():\n",
        "    if pred_val == j:\n",
        "        y_pred.append(i)\n",
        "\n",
        "# Print predicted label\n",
        "for predict in y_pred:\n",
        "    print(predict)"
      ],
      "metadata": {
        "id": "DbcEzzckWIT0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}